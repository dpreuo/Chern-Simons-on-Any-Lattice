\documentclass[11pt, oneside]{article} %Use "amsart" instead of "article" for AMSLaTeX format

\usepackage[left=2cm,top=3cm,right=2cm,bottom=4cm,head=1cm,a4paper]{geometry}    
\usepackage{graphicx} %Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\creflabelformat{equation}{#2\textup{#1}#3}
\usepackage{soul}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{braket}
\numberwithin{equation}{section}
\usepackage{bm}
\usepackage{appendix}
\usepackage[nobottomtitles*]{titlesec}
\usepackage{bbm}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage[
    % backend=biber, 
    % style=phys,
    % citestyle=numeric-comp,
    % biblabel=brackets, 
    % hyperref=true,
]{biblatex}

%Tikz stuff:
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,positioning} 
%\usepackage{pgfplots}

\newcommand{\noteAG}[1]{{\color[rgb]{0.,0.,0.8}[AG: #1]}}
\newcommand{\addAG}[1]{{\color[rgb]{0.,0.,0.8}{#1}}}

\input{macros.sty}

\bibliography{refs}
\usepackage{wrapfig}
\usepackage{caption}


\title{\textbf{Chern Simons on Any Lattice}}
\author{Peru}
\date{}

\begin{document}

\maketitle

\tableofcontents

\section*{A Quick Note On the Text}

These notes are very much a work in progress, in particular some sections are incorrect and need to be fixed. 

\begin{incorrect}
I have highlighted such sections in red boxes like this
\end{incorrect}

\section{Paper To-Do List}
Here are a few papers that I am still trying to understand that use these kinds of ideas (or modifications of them)
\begin{itemize}
    \item \cite{jacobson_modified_2023} Generalises these cup products and their related formalism for hypercubic chern simons theory -- I need to understand how they deal with coupling to an external field.
    \item \cite{demarco_compact_2021} also this paper by Wen and Demarco -- has an appendix where they go over the method.
\end{itemize}
    

\section{A Quick Chern Simons Recap}

Let us start by writing down the Chern-Simons Lagrangian in continuous space for a gauge field $A_\mu$ in $2+1$ dimensions coupled to some arbitrary matter field $J_\mu$,
\begin{align}\label{eqn:og_cs_lagrangian}
    \mathcal L &= \frac{k}{4\pi} 
    \varepsilon^{\alpha \beta \gamma}
    A_{\alpha} \partial_{\beta} A_{\gamma} - J_{\alpha }A^{\alpha}, \\ 
    & = \mathcal L_{CS} + \mathcal L_{\textup{coup}}.
\end{align}
We can also construct the field tensor $F_{\alpha \beta} = \partial_\alpha A_\beta - \partial_\beta A_\alpha$, which allows for the usual definition of the gauge-invariant electric and magnetic fields
\begin{align}
    E_j &= - \partial_i A_0 - \dot A_i, \\
    B &= \varepsilon_{ij}\partial_i A_j .
\end{align}
Now let us consider the effect of a gauge transformation
\begin{align}
    A_\mu \rightarrow A_\mu + \partial_\mu \chi.
\end{align}
Under this, the original Lagrangian (Eq.~\ref{eqn:og_cs_lagrangian}) changes according to 
\begin{align} \label{eqn:symmetric_change_lagrangian}
    \mathcal L_{CS} \rightarrow  L_{CS} + \frac k {4\pi}
    \varepsilon^{\alpha \beta \gamma} 
    \partial_\alpha \left [ 
    \chi \partial_\beta A_\gamma
    \right ],
\end{align}
where we have thrown away all terms with $\partial_\alpha \partial _\beta \cdot$ due to the asymmetry of $\varepsilon$. This term is a total derivative, so the action changes according to
\begin{align}
    S_{CS}[A] \rightarrow S_{CS}[A] + \frac k {4\pi} \int d^3x 
    \partial_\alpha \left [ 
    \varepsilon^{\alpha \beta \gamma} 
    \chi \partial_\beta A_\gamma
    \right ]
\end{align}
Thus, the only contribution to the action from the gauge transformation comes from a total derivative, which adds a boundary term. Provided that the integral is over a closed manifold, or is infinite and we make the assumption that the fields vanish at infinity, we have the guarantee that the Lagrangian is gauge invariant.

\subsection{The Same Story With Time and Space Separate} \label{sec:continuous_special_time}

The above argument has been written in a language where time and space are considered on an equal footing, however to make contact with the lattice in the next section we will want to discretise space but not time. Thus, let us split the gauge field into a time-like component $A_0$ and space-like component $\textbf A = (A_x, A_y)$. The Chern-Simons Lagrangian (in a Euclidean metric) can be re-expressed in the following form
\begin{align}\label{eqn:action_special_time_unfixed}
    \mathcal L = \frac{k}{4\pi} \epsilon_{ij} \left [  
    A_0  \partial_i A_j -
    A_i \dot A_j + 
    A_i \partial_j A_0
    \right ].
\end{align}
Let us define a scalar cross product, $\textbf A \times \textbf B = \varepsilon_{ij} A_i B_j $ in order to write this expression in the form
\begin{align}
    \mathcal L = \frac{k}{4\pi}
    \left [  
    A_0  B -
    \textbf {A} \times \dot {\textbf {A}} + 
    \textbf {A} \times \nabla A_0
    \right ].
\end{align}
\begin{shaded}
    Now we could invoke identity \ref{id:curl_cross} below to rewrite that last term in the form
    \begin{align}
        \textbf {A} \times \nabla A_0 = 
        - \nabla \times (A_0 \textbf A)
        +A_0 B.
    \end{align}
    Since the first term on the RHS above is expressed as a total derivative, it will vanish in the Lagrangian (subject to boundary conditions of course), so we can rewrite the Hamiltonian as Fradkin does in the form
    \begin{align}\label{eqn:fradkin_starting_point}
        \mathcal L = \frac{k}{4\pi}
        \left [  
        2A_0  B -
        \textbf {A} \times \dot {\textbf {A}}
        \right ].
    \end{align}
\end{shaded}

Now, let us again prove that this quantity has the right gauge-invariance properties. This might look kind of pointless but it will be helpful when we want to repeat the calculation in the lattice context. We apply a gauge transformation to our fields,
\begin{align}
    A_0 &\rightarrow A_0 + \dot \chi, \\
    \textbf A & \rightarrow A + \nabla \chi.
\end{align}
Under such a transformation, the magnetic field $B$ is invariant. Thus, the Lagrangian (\cref{eqn:action_special_time_unfixed}) changes according to
\begin{align}
    \mathcal L \rightarrow \mathcal L  &+ 
    \frac{k}{4\pi} \left [  
    \dot \chi  B 
    - (\nabla \chi \times \dot {\textbf A} 
    + \textbf A \times \nabla \dot \chi 
    + \nabla\chi \times \nabla \dot \chi)
    + (\nabla \chi \times \nabla A_0
    + \textbf A \times \nabla \dot \chi
    + \nabla \chi \times \nabla \dot \chi)
    \right ], \\
    & = 
    \frac{k}{4\pi} \left [  
    \dot \chi  B 
    - \nabla \chi \times \dot {\textbf A} 
    + \nabla \chi \times \nabla A_0
    \right ].\label{eqn:cont_intermediate_gauge}
\end{align}
We now apply identity \ref{id:curl_cross} to the second and third terms on the RHS,
\begin{align}
    \nabla \chi \times \dot {\textbf A} &= \nabla \times (\chi \dot{\textbf A}) - \chi \dot B, \\
    \nabla \chi \times \nabla  A_0 &= \nabla \times (\chi \nabla  A_0),
\end{align}
where in the last line we used the fact that $\nabla \times \nabla \cdot  = 0$. to get the final expression
\begin{align}
    \mathcal L \rightarrow \mathcal L  &+ \frac{k}{4\pi}
    \Big [
    \partial_0 (\chi B) + \nabla \times (\chi\nabla A_0 - \chi\dot {\textbf A})
    \Big ]
\end{align}
And so we see that, as expected, the change to the Lagrangian takes the form of a total derivative, consistent with \cref{eqn:symmetric_change_lagrangian}.

\begin{shaded}
    \textit{Identities used in the above discussion:} These are the properies we will need to preserve when we define the lattive version of curl and cross product in the next section if we want the above derivation to have a lattice equivalent. 
    \begin{enumerate}
    \item Antisymmetry of Cross Product:
        \begin{align} \label{id:cross_antisym}
            \textbf A \times  \textbf B = -
            \textbf B \times  \textbf A
        \end{align}
    
    \item Curl-Cross Product Rule:
        \begin{align} \label{id:curl_cross}
            \nabla \times (a \textbf{B}) = \nabla a \times \textbf{B} + a \nabla \times \textbf{B}
        \end{align}
    \item Curl-Grad = Div-Curl = 0:
    \begin{align}
        \nabla \times \nabla  f &= 0 \\
        \nabla \cdot \nabla \times \textbf  X &= 0 
    \end{align}
    
    \end{enumerate}
\end{shaded}

\section{Vectors, Scalars and Their Pseudo Cousins}

Before we go on to think about any specific process for discretisation, let us first consider what types of quantities we have making up our theory. This is something that one can often get quite far without paying attention to when working in the continuum, but all scalar and vector quantities can be separated into two types, polar (read: regular) and pseudo quantities, depending on how they transform under a transformation of the coordinate system
\begin{align}
    \textbf r' = R\textbf r
\end{align}
for some invertible matrix $R$. Regular quantities will always transform as
\begin{align}
    \textup{scalar: }\rho' &= \rho \\ 
    \textup{vector: } \textbf A' &=R \textbf  A
\end{align}
whereas pseudo-quantities will transform as
\begin{align}
    \textup{pseudo scalar: }\omega' &= \textup{det}R \cdot \omega \\ 
    \textup{pseudo vector: } \textbf B' &=\textup{det}R \cdot R\textbf  B
\end{align}
In particular, pseudo-quantities pick up a sign change under reflections. More generally, i.e.~in the language of differential forms, the various fields that one might construct exist in the space of m-forms, where $m \leq d$, the dimension of the space. 
\begin{shaded}
    Obviously there is a lot to say here -- differential geometry is a huge field. I'll just leave it at this for now and if more detail needs to be added that can happen later.
\end{shaded}

In the continuum it is easy to forget that the theory actually contains fields that live in 3 (or 4 if you're in 3D) completely different vector spaces, however when discretising the system this becomes incredibly important. This is because different fields appear on the lattice in different ways.
\begin{itemize}
    \item 0-fields (e.g. density, charge, probability amplitude) are scalars and live on the vertices.
    \item 1-fields (e.g. current and electric field) are vectors, so live on the edges.
    \item 2-fields (e.g. magnetic field and angular momentum) are pseudo scalars in 2D or pseudo vectors in 3D, so live on the faces. Of course in 3D these faces can have an orientation, which is where they get the voctor bit.
    \item 3-fields (e.g.~magnetic charge -- if it existed) lives in cells or 3-simplices.
\end{itemize}
Thus, when we take a continuum theory and try to put it on a lattice we will have to be mindful of exactly which field every quantity we are talking about lives in. Furthermore, there is no guarantee that an operation (for example the dot product, divergence, curl or cross product) looks the same acting on different fields! This adds a LOT of complication since we are going to have to come up with a definition of each of these operations that is specific to each field and even come up with operations that allows for notions like multiplication to be defined between fields. What does it mean to multiply a scalar that lives on vertices with a vector that lives on edges? What about a scalar that lives on faces with a vector on the edges? 

As we shall see, there is no unique answer to these questions, we will go through Fradkin and co's suggestion. And then we will detail a much more complete, much more satisfying general theory for discretising field theories and apply it to Chern-Simons theory for (hopefully) spectacular and very general results. 


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/maps_to_forms.pdf}
    \caption{The different types of vector spaces that fields can live in for 2 and 3 dimensional systems. Differential operators can be used to create objects that live either in a higher or lower space, depending on what operator you use. }
    \label{fig:maps_to_forms}
\end{figure}


\section{Fradkin's Discretisation}
\begin{shaded}
    Feel free to skip this section -- this project originally started by trying to understand the proposal given by Fradkin and his team in \cite{sun_fradkin_2015}. I came to the conclusion that their method was a truly awful way to do this. This section gives a very brief overview of what they do, but in the next section i will present a much more beautiful way of doing the same thing.
\end{shaded}
Our starting point for Fradkin's lattice Chern-Simons theory is the `separate time and space' Lagrangian, which we re-express as 
\begin{align}
    \mathcal L = \frac{k}{2\pi}
    \left [  
    A_0  B -\frac 12 
    \textbf {A} \times \dot {\textbf {A}} 
    \right ].
\end{align}
Before we discretise this, let us check exactly what type of field each of the quantities presented here is,
\begin{center}
\begin{tabular}{|c|c|}
\hline
    $A_0$ & 0-form \\ 
    $\textbf A$ & 1-form \\ 
    $\nabla A_0$ & 1-form \\ 
    $B$ & 2-form \\ 
    \hline
\end{tabular}
\end{center}
Thus, the field splits into two parts, a scalar field 
\begin{align}
    A_0 \rightarrow a_j
\end{align}
and a vector field defined on edges
\begin{align}
    \textbf A \rightarrow A_{jk}, \textup{ with } A_{jk} = -A_{kj}
\end{align}
where in both the above definitions $j$ labels vertices on the lattice. Thus, there are four things we will need to define for the Hamiltonian to be translated into a lattice language:

\textbf{Gradient: } This is the simplest, where the gradient of a generic 1-form $\phi_j$ is written as a straightforward finite difference
\begin{align}
    (\nabla \phi)_{jk} = \phi_j - \phi_k
\end{align}
where this quantity is only nonzero if the lattice contains an edge $j \rightarrow k$. We can also index edges with $(jk)\rightarrow e$ and this can be labelled as $(\nabla \phi)_e$.

\textbf{Curl: } Again, the definition that they use is not terribly controversial, the curl of a vector field is defined as the sum of the field around a plaquette such that all fields are taken counter clockwise
\begin{align}
    (\nabla \times \textbf A)_f = \sum_{(jk) \in f} A_{jk},
\end{align}
where $f$ labels the face.

\textbf{Product between a 0-form and 2-form: }This is where things get weird. Unlike in the continuum, you cannot straghtforwardly take a product of $B$ and $A_0$, since one lives on faces and the other lives on vertices. Fradkin and co.~come up with an unforgivable solution to this problem. They assign a \textit{vertex-face correspondence} such that every vertex is adjacent to a face with which it is paired. In particular they do this by assigning a matrix $M_{vf}$ which is 1 for a paired edge and face and zero otherwise. This is needlessly complex. Rather we will just label vertices and faces with the same set of indices such that the $j$\ts{th} vertex is always paired with the $j$\ts{th} face. Thus, the first term in the Hamiltonian may be written as 
\begin{align}
    a_j B_j\textup{, with } B_j = \sum_{(jk) \in f} A_{jk},
\end{align}

\textbf{Cross Product: } Finally we get to the cross product, where their definition is so profoundly arbitrary and ugly that I shall not sully this document with it. Suffice to say they encode the cross product in a matrix $K_{e,e'}$, the construction of which can only be described as a crime against humanity. If you want to suffer like I did, read the paper, however the long and short of it is that the substitution they make for the second term in \cref{eqn:fradkin_starting_point} is 
\begin{align}
    \int d^2x \textbf {A} \times \dot {\textbf {A}} 
    \rightarrow 
    A_e K_{e,e'} \dot A_{e'}
\end{align}
where we have compressed each edge $(jk)$ with a single label $e$ to avoid $K$ having four subscripts...

Thus, we may write down the final action as 
\begin{align}
    S = \frac k{2\pi} \int dt \left [ 
    a_j B_j - \frac 12 A_e K_{ee'} \dot A_{e'}
    \right ]
\end{align}

\subsection{Gauge Symmetry}

Finally, let's discuss gauge invariance and then we'll move on to our more general formalism. Under a gauge transformation, we change the fields according to
\begin{align}
    a_j &\rightarrow a_j + \dot \chi_j\\
    A_{jk} &\rightarrow A_{jk} + \chi_j - \chi_k \qquad (=(\nabla \chi)_{jk})
\end{align}
Bearing in mind that the $B$-field term is invariant, the change to the action becomes
\begin{align}
    S \rightarrow S + \frac k{2\pi} \int dt
    \left [ 
        \dot \chi_j B_j 
        - \frac 12 (\nabla \chi)_e K_{ee'} \dot A_e
        - \frac 12 A_e K_{ee'} (\nabla \dot \chi)_e 
        - \frac 12 (\nabla \chi)_e K_{ee'} (\nabla \dot \chi)_e 
    \right ]
\end{align}
now let's use integration by parts to move the dot over on the second term here to get
\begin{align}
    S \rightarrow S + \frac k{2\pi} \int dt
    \left [ 
        \dot \chi_j B_j 
        -  \dot A_e K_{ee'} (\nabla \chi)_e 
        - \frac 12 (\nabla \chi)_e K_{ee'} (\nabla \dot \chi)_e 
    \right ]
\end{align}
This expression should really be understood as the discrete version of \cref{eqn:cont_intermediate_gauge} in the previous section. Now what we need is a discrete form of the Curl-Cross identity (\cref{id:curl_cross}). Let us start with the integral form of the identity for a pair of arbitrary fields u and $\textbf X$, where we have discarded the part that is a total derivative:
\begin{align}
\begin{matrix}
      \int d^2x 
      \left ( -\textbf{V} \times \nabla u   + u \nabla \times \textbf{V} \right ) = 0\\
    \downarrow \\
    - V_e K_{ee'} (\nabla u)_e + u_j (\nabla \times \textbf V)_j = 0
\end{matrix}
\end{align}
Now it may not be obvious but this is exactly the `gauge invariance' condition that Fradkin derive in Eqn.~4.5 of their paper. They go to great lengths to prove that this identity holds in general.

Finally a summary of the rest of Fradkin's paper. After spending a few pages proving that their cross product is a cross product and their curl is a curl, they go on to prove that the theory generally behaves like CS theory, with flux attachment. They also show that the theory is local (which wouldn't be necessary if they had used a better definition of the fields).

\section{Simplices, Boundaries and Products Done Right}

Let us now try and fix Fradkin's formalism, and come up with a much more general, less arbitrary way of doing the same thing. To do this we will follow the rather beautiful introduction to the relevant concepts from algebraic topology \cite{schwalm_vector_1999}. The discussion in their paper is complete, and very clear, so we will just poach the results that we want to extract, and leave the proofs/justifications to be found in the source material. 

The first thing to know is that their construction is defined specifically for a theory discretised on a \textit{simplicial complex}. Nakahara provides a nice introduction to simplices and their respective complexes \cite{nakahara_geometry_2003}, for the current project the only thing this means is that all plaquettes must be triangles. In practice this is not a terrible restriction, because we can always add edges to a given lattice to ensure that it satisfies this condition, as shown in \cref{fig:simplices}a\footnote{Note that this is not a very unusual trick, see for example the triangulation used by Lieb in \cite{lieb_fluxes_2004}.}. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/simplces.pdf}
    \caption{
    \textbf{(a)} Adding edges to a lattice to make it a simplicial complex.
    \textbf{(b)} An example of a 0,1 and 2-simplex.
    }
    \label{fig:simplices}
\end{figure}

Let us start by defining what a simplex is. A 0-simplex, denoted by $(i)$ is simply a vertex on the lattice, labelled at position $i$. To construct higher form simplices, we can add a vertex to a given simplex, provided that it is adjacent to all the vertices in that simplex. I.e.~$(i,j)$ is a 1-simplex which labels an edge from site $i$ to site $j$, whereas $(i,j,k)$ labels a triangle from $i$ to $j$ to $k$, as shown in \cref{fig:simplices}b. Note that all simplices have the property that they are antisymmetric under permutation of thier indices. That is, for a general permutation $\mathcal P$
\begin{align}
    (P(j), P(j), P(k), ..., P(n)) = \textup{sgn}(P) (i,j,k,...,n).
\end{align}

Now we can use the set of all $n$-simplices as a basis for constructing fields! Here we define an $n$-field as an element of the vector space spanned by all $n$-simplices:
\begin{align}
    \textup{0-form: } \phi &= \sum_{[i]} (i) \phi_{i}  \\ 
    \textup{1-form: } \alpha &= \sum_{[i,j]} (i,j) \alpha_{ij} \\ 
    \textup{2-form: } \beta &= \sum_{[i,j,k]}(i,j,k) \beta_{ijk} 
\end{align}
where the general field term must always be antisymmetric under permutation of the indices, eg:
\begin{align}
    \beta_{ijk} = - \beta_{jik}
\end{align}

\begin{shaded}
    It might make sense to include a factor of $\frac 1{n!}$ in the field term of a general $n$-form... This would line up nicely with the continuous theory of differential forms. Let's see later if we want to introduce it.
\end{shaded}

\begin{incorrect}
    Another detail Ihave glossed over -- really one should distinguish between chains and cochains. 
\end{incorrect}

\subsection{Derivative Operators}

Now let us define a notion of `differentiation' on these fields. There are two derivative operators that we will introduce, parachuted in directly from the mathematics of differential forms: the boundary $\partial$ and the coboundary $d$. These operators act directly on the simplices, and effectively ignore the field degree of freedom. In essence, when acted on a generic $n$-field $\gamma$
\begin{align}
    \partial \gamma = \partial \sum_{[i,j,k,...,n]} \gamma_{ijk...n} \partial (i,j,k,...,n),
\end{align}
with the same property for $d$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/boundaries.pdf}
    \caption{
    \textbf{(a)} The action of the boundary operator on a general 0, 1 and 2-simplex.
    \textbf{(b)} The action of the coboundary operator on a the same set of simplices.
    }
    \label{fig:boundaries}
\end{figure}
\subsubsection{Boundary}
The boundary is a map from $n$-simplices to $(n-1)$-simplices. The operation is simple, 
\begin{align}
    \partial (i,j,k,l,...) = (j,k,l,...) - (i,k,l,...) + (i,j,l,...) - (i,j,k,...) + ...
\end{align}
That is, we form a sum of every simplex formed by dropping one vertex, with the sign determined by whether the position of the deleted vertex is odd or even. Let us calculate a few examples, which are shown visually in \cref{fig:boundaries},
\begin{align}
    \partial (i) &= 0, \\
    \partial (i,j) &= (j) - (i), \\
    \partial (i,j,k) &= (j,k) - (i,k) + (i,j).
\end{align}
We can also calculate the effect of this operator on our fields. Firstly, it can be seen that the boundary acting on a 0-field always vanishes,
\begin{align}
    \partial \phi & = 0.
\end{align}
Next, the boundary of a 1-field is given by
\begin{align}
    \partial \alpha & = \sum_{[i,j]}
    \left [ 
    (j) - (i)
    \right ]
    \alpha_{ij} , \\
    & = \sum_{[i]} (i) \sum_{[j] \rightarrow [i]}
    \alpha_{ji},
\end{align}
where the second sum is over all vertices $j$ adjacent to $i$. This is rather promising -- if you picture $\alpha_{ij}$ as something like a current (or electric field) then this would tell you how much current is entering or exiting site $i$ from its neighbours! I.e.~this looks a lot like a divergence! Let us carry on by calculating the case for a 2-field,
\begin{align}
    \partial \beta &= \sum_{[i,j,k]} 
    \left [
    (j,k) - (i,k) + (i,j)
    \right ]
    \beta_{ijk} \\
    & = \sum_{[i,j])} (i,j) 
    \sum_{[k]\rightarrow [i,j]}
    \beta_{ijk},
\end{align}
where the second sum is over all vertices $k$ that are adjacent to \textit{both} $i$ and $j$. This is even more promising! Imagining $\beta$ as a quantity like vorticity, this calculates exactly the current that would be generated on the edge $(i,j)$ as a consequence of difference in vorticity in the two adjacent triangles. I.e. this looks a lot like a curl! The boundary of a general $n$-field may be calculated using the two equivalent forms
\begin{align}
    \partial \gamma &=\sum_{[v_0, ... ,v_{n}]} \gamma_{ v_0, ... ,v_{n}}
    \sum_{m = 0}^n (-1)^m
    (v_0, ...,\hat v_m,... ,v_{n}) 
    \\
    &=\sum_{[v_1, ... ,v_{n}])} (v_1, ... ,v_{n}) 
    \sum_{[v_0]\rightarrow [v_0, ... ,v_{n}]}
    \gamma_{v_0, v_1, ... ,v_{n}}.
\end{align}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/maps_to_forms_boundaries.pdf}
    \caption{We repeat the hierarchy of operations depicted in \cref{fig:maps_to_forms}, now showing the action of the boundary and coboundary operators.}
    \label{fig:maps_to_forms_boundaries}
\end{figure}



\subsubsection{Coboundary}

Now we turn our attention to the coboundary operator $d$, which is defined as 
\begin{align}
    d(i,j,k,...,n) = \sum_{[u]\rightarrow [i,j,k,...,n]}(u,i,j,k,...,n).
\end{align}
Let's again play the game of seeing how this operator acts on our fields. Lets start with a 0-field
\begin{align}
    d\phi &= \sum_{[j]} \sum_{[i]\rightarrow [j]} \phi_j (i,j) \\
    & = \sum_{[i,j]}(\phi_j - \phi_i) (i,j)
\end{align}
As before, we might notice that this operator acts a bit like a discrete version of the grad operator! Now looking at 1-fields we find that
\begin{align}
    d\alpha &= \sum_{[j,k]} \sum_{[i]\rightarrow [j,k]} \alpha_{jk} (i,j,k), \\
    & = \sum_{[i,j,k]}(\alpha_{ij} - \alpha_{ik} + \alpha_{jk}) (i,j,k)
\end{align}
Again, treating $\alpha_{jk}$ as a current this looks a lot like a curl operator! In fact, by considering \cref{fig:maps_to_forms} we can see that the boundary and coboundary operators correspond exactly to the differential operators that can map continuous fields between the different `levels'. The coboundary of a general $n$-field may be calculated using the two equivalent forms:
\begin{align}
    d\gamma
    & = \sum_{[v_1, ... , v_{n+1}]}\alpha_{v_0...v_n}\sum_{[v_0]\rightarrow [v_1, ... , v_{n+1}]}
    (v_0, ... , v_{n+1})
    \\
    &= \sum_{[v_0, ... , v_{n+1}]}
    (v_0, ... , v_{n+1})
    \sum_{m = 0}^{n+1}(-1)^m
    \gamma_{v_0...\hat v_m...v_{n+1}},
\end{align}
where $\hat v$ indicates that this index is omitted.


\subsubsection{Inner Product and Stokes Theorem}

To complete this discussion of our discrete calculus, we must also define a discrete analog of integration. To do this let us introduce an ordered $n$-chain $\mathcal C$ as the discrete analog of an oriented arc or surface. Chains are also organised into a hierarchy of $n$-forms
\begin{align}
    \mathcal C_0 &= \sum_{[i]\in \mathcal C} (i), \\
    \mathcal C_1 &= \sum_{[i,j]\in \mathcal C} (i,j), \\
    \mathcal C_2 &= \sum_{[i,j,k]\in \mathcal C} (i,j,k).
\end{align}
We must also define an inner product on forms, given by
\begin{align}
    \inner{(i),(l)} & = \delta_{i,l},\\
    \inner{(i,j),(l,m)} & = \delta_{i,l}\delta_{j,m} -\delta_{i,m}\delta_{j,l}.
\end{align}
for a general $m$-form the inner product is given by
\begin{align}\label{eqn:inner_prod_determinant}
    \inner{(i,j,k,...),(l,m,n,...)} = \textup{det}\begin{pmatrix}
        \delta_{il} & \delta_{im} & \delta_{in}\\
        \delta_{jl} & \delta_{jm} & \delta_{jn}& \cdots\\
        \delta_{ikl} & \delta_{ikm} & \delta_{kn}\\
        & \vdots & & \ddots
    \end{pmatrix}.
\end{align}
This looks really complicated but it actually isn't. All we're saying is that the product is only nonzero if $(i,j,k,...)$ and $(l,m,n,...)$ contain exactly the same vertices as one another, but if they are ordered differently then we might pick up a factor of $-1$ from reordering the indices to make the two forms identical. 

Now we can define the analog of a line intergal, as the inner product of a 1-chain and a 1-field:
\begin{align}
    \inner {\alpha, C} = \sum_{[i,j]\in C} \alpha_{ij}.
\end{align}
Now look at 
\begin{align}
    \inner {d\phi, C} &= \sum_{[i,j]\in C}(\phi_j - \phi_i),\\
    & = \phi_b - \phi_a.
\end{align}
where vertex $b$ is where the chain ends and $a$ is where the chain starts. Thus, we can see immediately that
\begin{align}
    \inner {d \phi, C} = \inner {\phi, \partial C}
\end{align}
In fact, this expression holds for any general pair consisting of a $n$-form $\xi$ and an $(n+1)$-form $\zeta$
\begin{align}
    \inner {d\xi, \zeta} = \inner {\xi, \partial \zeta}
\end{align}
and we have the discrete form of Stokes' law, in a completely elegant form.

\subsection{Products}

This is the point where \cite{schwalm_vector_1999} loses my interest a bit. Their next step is to write down a list of about ten vector calculus identities. Things like 
\begin{align}
    \nabla \cdot (\rho \textbf A)
    & = \nabla \rho \textbf A + \rho \nabla\cdot \textbf A,\\
    \nabla \cdot (\textbf A \times  \textbf B)
    & = 
    \nabla \times \textbf A\cdot  \textbf B - 
     \textbf A\cdot  \nabla \times\textbf B ,
\end{align}
basically all the super standard vector identities. Obviously, for the discrete field theory to look like what we expect, we want our definitions of products to be defined in such a way that the `differential operators' discussed above follow the correct generalisation of the chain rule. From their analysis, it feels like they don't really find a universal rule for a product of chains -- or if they did they wrote it really shittily. Actually one exists and it took me a day to work it out.

Let's start by writing down a few of the identities that are presented in \cite{schwalm_vector_1999}:
\begin{itemize}
    \item \textbf {Scalar Multiplication} 
     basically amounts to taking a product between a 0-form and a (something else)-form. Let's put down the general identities for the product between a 0-form $\phi$ and an $n$-form $\xi$,
    \begin{align}\begin{aligned}
        \phi \xi &= 
        \left (
        \sum_{[i] } \phi_i (i)
        \right )
        \left (
        \sum_{[i,j,k,...,m] }\xi_{ijk...m} (i,j,k,...,m)
        \right ) \\
        &=\sum_{[i,j,k,...,m]}
        \frac 1n (\phi_i + \phi_j + \phi_k + ... + \phi_m)
        \xi_{ijk...m} (i,j,k,...,m)
    \end{aligned}
    \end{align}

    \item \textbf {Cross Products} 
     between two 1-forms create a 2-form. We will define the cross product between two 1-fields $\alpha$ and $\sigma$ as 
    \begin{align}
        \alpha \times \sigma = 
        \sum_{[i,j,k]} \frac 16 \left [ 
        (\alpha_{ki} +\alpha_{kj} )\sigma_{ij}
        + (\alpha_{ij} +\alpha_{ik} )\sigma_{jk}
        + (\alpha_{ji} +\alpha_{jk} )\sigma_{ki}
        \right ](i,j,k)
    \end{align}

    \item \textbf {Dot Products} between two 1-forms are written as 
    \begin{align}
        \alpha \cdot \beta = \sum_{[i]} 
        \sum_{[j]\rightarrow [i]} 
        \frac 12 \alpha_{ij}\beta_{ij} (i)
    \end{align}
    and take you to a 0-form
\end{itemize}
To find a nice general theory for this, we will construct -- in analogy with the derivative operators -- two different product operators that either take you up or down in the hierarchy of chains

\begin{shaded}
    The next sections borrow heavily from the \textit{Serious Algebraic Topology} literature. In particular, this stuff is simplified from staring confusedly at Hatcher's book \cite{hatcher_algebraic_2002}, in particular the chapters on homology and cohomology. In general, since we work with MUCH less pathological spaces than mathematicians generally do, we have a lot of space to really simplify their definitions. I.e. did you know that a cochain and a chain is basically just the same thing in a finite compact space? Took me ages to figure that out lol ;) 
    
    Note, I borrow the names for these operators from their names in algebraic topology. 
\end{shaded}

\subsubsection{The Cup Product}

Let us develop an argument for the form of a product first. Following the general form of the vector calculus identities, we know that we want to take a product of an $n$-field and an $m$-field to create an $(n+m)$-field,
\begin{align}
    \alpha^n \smile \beta^m = \gamma ^{n+m}.
\end{align}
In general, an $n$-simplex has $(n+1)$ vertices in its definition, so we expect the product of an $n$ and $m$ simplex to have $(n+m+1)$ vertices in its definition. 

Thus, one might start by defining a product that looks a bit like matrix multiplication
\begin{align} \label{eqn:simplex_otimes}
    (v_0, v_1,  ... ,v_n)\otimes (w_0, w_1, ... ,w_m) = \delta_{v_n, w_0} (v_0, v_1, ... ,v_n, w_1, ... ,w_m),
\end{align}
however such a product is not sufficient. This is because this operator has the undesirable property that 
\begin{align}
    (i,j) \otimes (k,l) \neq - (i,j) \otimes (l,k).
\end{align}
\begin{incorrect}
    
Thus, in order to construct a viable product operator, we will have to actively anti-symmetrise over every possible ordering of the indices in each simplex
\begin{align}\label{eqn:simplex_cup_product}
    (v_0, v_1, ... ,v_n) \smile (w_0, w_1, ... ,w_m) 
    &= 
    \sum_{P,Q} 
    \frac{\sgn(P)\sgn(Q) }{(n+m+1)!}
    (v_{p_0}, v_{p_1}, ... ,v_{p_n}) 
    \otimes 
    (w_{q_0}, w_{q_1}, ... ,w_{q_m}),\\
    & = \sum_{P,Q} 
    \frac{\sgn(P)\sgn(Q) }{(n+m+1)!}
    \delta_{v_{p_n}w_{q_0}}
    (v_{p_0}, v_{p_1}, ... ,v_{p_n}, w_{q_1}, ... ,w_{q_m})
\end{align}
where $P$ and $Q$ are two different permutations, and the reason for the normalisation will become clear in the next paragraph. 
\end{incorrect}

\begin{shaded}
    A little bit like the inner product formula given above (\cref{eqn:inner_prod_determinant}), this looks much more complex than it actually is. To see why first notice that in \cref{eqn:simplex_otimes}, the product we defined with $\otimes$ will vanish unless the $v$ and $w$ simplices share exactly one common vertex -- the last vertex in $v$ and the first in $w$. Of course if $v_n$ and $w_0$ differ we vanish due to the $\delta$-function. However if any other vertices are shared by both simplices, the resultant big simplex $(v,...w,...)$ will have a repeat index and so must vanish by antisymmetry under reordering! Thus we may assume that any product basically looks like
    \begin{align}
        (...,v_j, v, v_{j+1} ,...) \smile (...,w_j, v, w_{j+1} ,...),
    \end{align}
    where all the labels with subscripts are distinct. Now all we must do is swap indices around until the common index is in the right place:
    \begin{align}
        = (\pm 1) (...,v_j, v_{j+1} ,...,v ) \smile (v,...,w_j, w_{j+1} ,...),
    \end{align}
    and the product is formed by joining the two simplices together into 
    \begin{align}
    = (\pm 1)(...,v_j, v_{j+1} ,...,v, ...,w_j, w_{j+1} ,...).
    \end{align}    
    Simple!
\end{shaded}

Now, let us look at the effect of this term on a product of two fields, 
\begin{align}
    \alpha^n = \sum_{[v_0, ..., v_n]}\alpha_{v_0 ...v_n}( v_0, ..., v_n), 
    \textup{ and } 
    \beta^m = \sum_{[w_0, ..., w_m]}\beta_{w_0 ...w_m}( w_0, ..., w_m),
\end{align} 
which can be written in the form
\begin{align}\label{eqn:field_cup_product}
    \alpha^n \smile \beta^m = \sum_{[v_0, ...,v_{n+m}]} 
    \sum_{P} \frac{\sgn(P)}{(n+m+1)!} \alpha_{p_0 ...p_n}\beta_{p_n ...p_{n+m}} (v_0, ...v_n,...,v_{n+m}).
\end{align}
Finally, note that this cup product satisfies
\begin{align}
    \alpha^n \smile \beta^m = (-1)^{nm} \beta^m \smile \alpha^n.
\end{align}

\peru{\cref{eqn:simplex_cup_product} is not correct -- but \cref{eqn:field_cup_product} is correct. The exact form of the product acting on individual simplices is rather complex -- see section 3B (the K\"unneth formula) of Hatcher to understand why...
}

Let us apply this product and see what the result is when different fields are multiplied. In two dimensions there are two broad cases. Let's look at them individually.

\begin{itemize}
    \item {\bf Multiplication With a 0-field}
When multiplying a 0-field, $\phi^0$, with an $m$-field $\beta^m$, the sum over permutations in expression \ref{eqn:field_cup_product} takes the form
\begin{align}
    \sum_{P} \frac{\sgn(P)}{(m+1)!} \phi_{p_0}\beta_{p_0 ...p_{m}} = \frac{1}{m+1}(\phi_{v_0}+\phi_{v_1}+ ... +\phi_{v_m})
    \beta_{v_0...v_m},
\end{align}
so the general form of such a multiplication is
\begin{align}
    \phi^0 \smile \beta^m = \sum_{[v_0, ... ,v_m]}\frac 1 {m+1}
    (\phi_{v_0}+\phi_{v_1}+ ... +\phi_{v_m})
    \beta_{v_0...v_m}
    (v_0,...,v_{m}).
\end{align}
That is, the product is just a weighted sum of the 0-field over every $m$-simplex. Simple. Thus we see that here the cap product plays the role of multiplication between a scalar field and any other field. 

\item{\bf Multiplication With a 1-field}
In 2D, there is only one other cup product we can take that is non-vanishing, the product of two 1-fields to produce a 2-field. Given what we know about taking products of vectors (1-fields) to get pseudo-vectors (2-fields), we expect that this should give us the formula for a cross product. writing out all terms explicitly, we arrive at the following expression
\begin{align}
    \alpha^1 \smile \beta^1 = \sum_{[i,j,k]}\frac{1}{6}
    (
    \alpha_{ij} \beta_{jk}
    +\alpha_{jk} \beta_{ki}
    +\alpha_{ki} \beta_{ij}
    -\alpha_{ik} \beta_{kj}
    -\alpha_{kj} \beta_{ji}
    -\alpha_{ji} \beta_{ik}
    )
    (i,j,k)
\end{align}
Again, this looks complex until you realise all the positive and negative terms are just the six positive and negative permutations of $i,j,k$.
\end{itemize}

\subsubsection{The Cap Product}
There's still one product missing -- the dot product -- which takes a pair of 1-fields or a pair of 2-fields to a 0-field. Again borrowing from algebraic geometry, this can be constructed using an equivalent to their cap product, which takes a product of an $n$-field and an $m$-field to produce an $(n-m)$-field:
\begin{align}
    \alpha^n \frown \beta^m = \gamma ^{n-m}
\end{align}
Lets try and build it as we did before, starting with a guess, if we want to take a product of an $n$-simplex and an $m$-simplex to produce an $(n-m)$-simplex, the most obvious way to do it is basically by identifying the `first' $m+1$ vertices shared by both simplices
\begin{align}
    (v_0, v_1, ... ,v_n)\odot(w_0, w_1, ... ,w_m) 
    = 
    \delta_{v_0 w_0}
    \delta_{v_1 w_1}
    ...
    \delta_{v_m w_m}
    (v_m,...,v_n)
\end{align}
Again, we run into the same problems as before, that is, this operator is not antisymmetric under a swap of its indices, for example
\begin{align}
    (i,j,k)\odot (l,m) \neq - (i,k,j) \odot (l,m).
\end{align}
\begin{incorrect}
    
To fix this, we must again antisymmetrise over all possible orientations of the higher order simplex
\begin{align}
    (v_0, v_1, ... ,v_n)\frown(w_0, w_1, ... ,w_m) 
    &=
    \sum_{P} 
    \frac{\sgn(P) }{(n+1)!}
    (v_{p_0}, v_{p_1}, ... ,v_{p_n}) 
    \odot 
    (w_0, w_1, ... ,w_m),\\
    &= 
    \sum_{P} 
    \frac{\sgn(P)}{(n+1)!}
    \delta_{p_0 w_0}
    \delta_{p_1 w_1}
    ...
    \delta_{p_m w_m}
    (p_{m}, p_{m+1},...,p_n).
\end{align}
\peru{Again, I need to check that this expression is correct -- I know that \cref{eqn:field_cap_product} is right -- but there's definitely a mistake here in the above expression.}
\end{incorrect}
Now, let us look at the effect of this on a product of two fields,\footnote{note that the sign of the permutations has been absorbed by the antisymmetric properties of the tensors $\alpha_{p...}$ and $\beta_{p...}$.}
\begin{align}\label{eqn:field_cap_product}
    \alpha^n \frown \beta^m = \sum_{[v_0, ...,v_{n}]} 
    \sum_{P} \frac{\sgn(P)}{(n+1)!} \alpha_{p_0 ...p_n}\beta_{v_0 ...v_m} (v_m,...,v_{n}),
\end{align}

Let us apply this product and see what the result is when different fields are multiplied. Unlike the cap product case, there are a few more cases to consider

\begin{itemize}
    \item {\bf Multiplication With a 0-field}
When multiplying an $n$-field $\alpha^n$ with a 0-field, $\phi^0$, the sum over permutations in expression \ref{eqn:field_cup_product} takes the form
\begin{align}
    \sum_{P} \frac{1}{(n+1)!} \phi_{p_0}\beta_{p_0 ...p_{n}} = \frac{1}{n+1}(\phi_{v_0}+\phi_{v_1}+ ... +\phi_{v_n})
    \beta_{v_0...v_n},
\end{align}
so the general form of such a multiplication is
\begin{align}
    \beta^n \frown \phi^0 = \sum_{[v_0, ... ,v_m]}\frac 1 {n+1}
    (\phi_{v_0}+\phi_{v_1}+ ... +\phi_{v_n})
    \beta_{v_0...v_n}
    (v_0,...,v_{n}).
\end{align}
That is, this looks exactly like the cup product! Basically, scalar multiplication is boring.

\item{\bf Multiplying an $n$-field with an $n$-field}
Here we will basically be looking for something like a dot product, which always maps vector fields back to scalar fields. Plugging in \cref{eqn:field_cap_product}, we see that the product of two 1-fields is
\begin{align}
    \alpha^1 \frown \beta^1 &= \sum_{[i,j]}\frac 12 \alpha_{ij}\beta_{ij} \left [ (i) + (j)\right ], \\
    &= \sum_{[i]} \sum_{[j]\rightarrow [i]}\frac 12 \alpha_{ij}\beta_{ij} (i).
\end{align}
Similarly, the cap product of a pair of 2-fields is given by
\begin{align}
    \alpha^2 \frown \beta^2 = \sum_{[i,j,k]} \frac 13 \alpha_{ijk} \beta_{ijk} \left [ (i) + (j) + (k) \right ]
\end{align}


\item{\bf Multiplying a $2$-field with a $1$-field}
Honestly, when will we ever need this -- this term multiplies a B and E field to get an object that behaves like an E-field -- basically a cross product between 2 and 1-fields. I'll fill this in if i need it but honestly I'm sick of thinking really hard about where these permutation signs go...

\end{itemize}

\subsubsection{A Duality between the cup and cap product}
The cap and cup product satisfy a nice duality, reminiscent of Stokes' theorem. For an $n$-field $\alpha$, an $m$-field $\beta$ and an $(n+m)$-field $\gamma$,
\begin{align}
    \inner{\gamma, \alpha \smile \beta}
    =
    \inner{\gamma \frown \alpha , \beta }.
\end{align}
The proof is from directly evaluating the inner product
\begin{align}
    \inner{ \gamma,\alpha \smile \beta} &=
    \sum_{[v_0 ... v_{n+m}]} \gamma_{v_0,...v_{n+m}}
    \sum_{P}\frac{\sgn(P)}{(n+m+1)!}
    \alpha_{p_0...p_n}\beta_{p_n...p_{n+m}} \\
    &= \sum_{[v_0 ... v_{n+m}]}
    \beta_{v_n...v_{n+m}}
    \sum_{P}\frac{\sgn(P)}{(n+m+1)!}
    \gamma_{p_0,...p_{n+m}}
    \alpha_{v_0...v_n}\\
    &= \inner{\gamma \frown \alpha , \beta }
\end{align}

\subsection{The Chain Rule}
Finally, we have a definition of two derivatives, $\partial$ and $d$, and a definition of two products, $\smile$ and $\frown$. In Appendix \ref{apx:chain_proof} we prove the following two relations hold

\begin{align}
    d (\alpha^n \smile \beta^m) &= 
    d \alpha^n \smile \beta^m + 
    (-1)^n \alpha^n \smile d\beta^m,\\
    \partial (\alpha^n \frown \beta^m) &= 
    (-1)^m 
    \left (
    \partial \alpha^n \frown \beta^m - 
    \alpha^n \frown d\beta^m
    \right ).
\end{align}

\section{Chern Simons on Any Lattice}

Now let us apply the developed discretisation procedure to create  a lattice Chern Simons theory. We start as ever by restating the continuous-space action
\begin{align}
    S = \frac{k}{4\pi}
    \int dt \int d^2x
    \left [  
    A_0  B -
    \textbf {A} \times \dot {\textbf {A}} -
    \nabla A_0  \times  \textbf {A}
    \right ].
\end{align}
Let us make our substitutions for the simplicial versions of these fields, which involve a 0-field $\phi$ and a 1-field $\alpha$
\begin{align}
    A_0 \rightarrow \phi = \sum_{[i]}\phi_i(i),\\
    \textbf A \rightarrow \alpha = \sum_{[i,j]}\alpha_{ij} (i,j).
\end{align}
Next, we can calculate the two derivative forms in the expression
\begin{align}
    \nabla A_0 \rightarrow d\phi &= \sum_{[i,j]}(\phi_j - \phi_i) (i,j),\\
    B \rightarrow d\alpha  &= \sum_{[i,j,k]} ( \alpha_{ij} + \alpha_{jk} + \alpha_{ki}) (i,j,k)
\end{align}
Therefore we can write the integrand in the form
\begin{align}
    \mathcal L  = \frac k{4\pi} \left (  \phi \smile d\alpha 
    - \alpha \smile \dot \alpha 
    -d\phi \smile \alpha
    \right ).
\end{align}
This integrand takes the form of a 2-field, therefore to write the integral, one must define a 2-chain that covers the whole complex (i.e. all of space),
\begin{align}
    C = \sum_{[i,j,k]} (i,j,k)
\end{align}
we can write the action in the form
\begin{align}
    S = \frac k{4\pi} \int dt \inner{C, 
    \phi \smile d\alpha 
    - \alpha \smile \dot \alpha 
    -d\phi \smile \alpha
    }
\end{align}
% now we can clean this up with the chain rule, since we know that
% \begin{align}
%     d(\phi \smile \alpha) = 
%     d\phi \smile \alpha + 
%     \phi \smile d\alpha
% \end{align}
% we may write this as 
% \begin{align}
%     S = \frac{k}{2\pi} \int dt \inner{C,
%     \phi \smile d\alpha 
%     - \frac 12 \alpha \smile \dot \alpha
%     } -
%     \frac{k}{4\pi} \int dt \inner{C,
%     d(\phi \smile \alpha )
%     },
% \end{align}
% where we can use Stokes' theorem to cancel out the last term since
% \begin{align}
%     \inner{C,
%     d(\phi \smile \alpha )
%     } = \inner{\partial C,
%     \phi \smile \alpha 
%     },
% \end{align}
% which vanishes if we assume we are integrating over the whole space (i.e. $C$ has no boundary).


\subsection{Gauge Transformations}

Let us consider the effect of a gauge transformation, performed by making the following changes to the fields
\begin{align}
    \phi &\rightarrow \phi + \dot \chi, \\
    \alpha &\rightarrow \alpha + d\chi,
\end{align}
where $\chi$ is some arbitrary 0-field. Applying this transformation to the action we arrive at
\begin{align}
    \mathcal L &\rightarrow \mathcal L + \frac{k}{2\pi} \left [
    \dot \chi \smile d\alpha
    - d\chi \smile \dot \alpha
    + d\chi \smile d\phi
    \right]
\end{align}
Now we can proceed exactly as we did in \textsection\ref{sec:continuous_special_time}, using the following two chain rule identities
\begin{align}
    d(\chi \smile \dot \alpha) &= d\chi \smile \dot\alpha + \chi \smile d\dot\alpha, \\
    d(\chi \smile d \phi) &= d\chi\smile d \phi,
\end{align}
where the latter identity uses $d^2 = 0$, to get the following
\begin{align}
    \mathcal L &\rightarrow \mathcal L + \frac{k}{2\pi} \left [
    \partial_t(\chi\smile d\alpha)
    + d(\chi\smile d \phi - \chi\smile \dot \alpha)
    \right].
\end{align}
This term is a total derivative, so we have verified gauge invariance!

\subsection{Coupling to a Current}

Now we would like to introduce a coupling to an external current, that is we would like to add in the term
\begin{align}
    A_\mu J^\mu = A_0 J_0 + \textbf A \cdot \textbf J
\end{align}

\begin{incorrect}
    Many questions here -- clearly for this term to have the same form-type (the rest of the Lagrangian is a 2-form) this term must also be a 2-form. That implies that $J_0$ has to be a 2-form. This would make sense if we assumed that the gauge field lives on the triangulated lattice and the matter field lives on the dual lattice -- which would be a three coordinated lattice. This is very tempting when one is thinking about applying this to the Kitaev Honeycomb model.

    Making the identification:
    \begin{align}
        J_0 \rightarrow \mu &= \sum_{[i,j,k]}\mu_{ijk}(i,j,k) \\
        \textbf J \rightarrow \xi &= \sum_{[j,k]}\xi_{jk} (j,k)
    \end{align}
    Then, the temptation is to write the term as 
    \begin{align}
        A_\mu J^\mu \rightarrow \phi \smile \mu + \alpha \smile \xi
    \end{align}
    I suspect this is not correct ... the first term looks alright but the second term looks suspicious, since we have identified a dot product with $\smile$ which generally describes cross products between pairs of $1$-fields. 

    I think the solution here is to think carefully about the dual lattice -- which corresponds to exchanging $m$-fields for $(d-m)$-fields. A cup and cap product on the regular lattice will induce a dual product operation on the dual lattice. 

    Still a lot to think about.
    
\end{incorrect}

\printbibliography

\input{appendix}

\end{document}
